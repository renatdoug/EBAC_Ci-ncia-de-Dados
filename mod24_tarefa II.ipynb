{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6543c086-b1d4-44f0-a562-3a23ba36d606",
   "metadata": {},
   "source": [
    "1 - Cite 5 diferenças entre o AdaBoost e o GBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1301e2-38c9-49c7-8e78-6ac853284980",
   "metadata": {},
   "source": [
    "1. **Algoritmo de base:**\n",
    "   - AdaBoost é um algoritmo de boosting que combina vários classificadores fracos (conhecidas como stumps) sequencialmente, dando mais peso às amostras mal classificadas em cada iteração.\n",
    "   - GBM é uma técnica de boosting que também combina vários árvores de decisções, mas em vez de atribuir pesos às amostras, ajusta os pesos dos erros residuais para minimizar esses erros em cada iteração.\n",
    "\n",
    "2. **Pesos das amostras:**\n",
    "   - Em AdaBoost, as amostras mal classificadas recebem pesos maiores durante o treinamento, o que ajuda o modelo a se concentrar mais nas amostras difíceis de classificar.\n",
    "   - Em GBM, o foco está nos erros residuais. Cada novo classificador é treinado para corrigir os erros cometidos pelos classificadores anteriores, ajustando os pesos dos erros residuais em cada iteração.\n",
    "\n",
    "3. **Abordagem de ajuste dos erros:**\n",
    "   - AdaBoost usa uma abordagem de ajuste adaptativo, onde o modelo se concentra nas amostras mal classificadas, ajustando gradualmente seus pesos para corrigir esses erros.\n",
    "   - GBM usa uma abordagem de ajuste aditivo, onde cada novo classificador é adicionado ao modelo para corrigir os erros residuais dos classificadores anteriores, criando um modelo mais complexo e preciso ao longo do tempo.\n",
    "\n",
    "4. **Processo de treinamento:**\n",
    "   - AdaBoost treina os classificadores fracos sequencialmente, dando mais peso às amostras mal classificadas em cada iteração.\n",
    "   - GBM treina os classificadores fracos de forma sequencial, ajustando os pesos dos erros residuais em cada iteração e adicionando classificadores que se concentram nos erros residuais restantes.\n",
    "\n",
    "5. **Sensibilidade a outliers:**\n",
    "   - AdaBoost é sensível a outliers, pois atribui pesos às amostras mal classificadas para corrigir erros, o que pode levar a um ajuste excessivo se os outliers não forem tratados adequadamente.\n",
    "   - GBM também pode ser sensível a outliers, especialmente se não forem tratados adequadamente, pois os erros residuais podem ser amplificados em cada iteração.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56074e88-aae2-4f94-a3db-9577584038b5",
   "metadata": {},
   "source": [
    "2 -Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e9d5f7-b6ca-4fd0-8074-99d2da461802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc6dbc-64f2-45f0-98b9-9485a393d9e9",
   "metadata": {},
   "source": [
    "3 - Cite 5 Hyperparametros importantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445e86d-d0c3-4787-b927-f8a927a01842",
   "metadata": {},
   "source": [
    "**sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)**\n",
    "\n",
    "\n",
    "1. **n_estimators:**\n",
    "   - Reescrita: Este hiperparâmetro especifica o número de estimadores (classificadores fracos, como árvores de decisão) que serão combinados para formar o modelo final do GBM.\n",
    "\n",
    "2. **learning_rate:**\n",
    "   - Reescrita: A learning_rate controla a contribuição de cada estimador para a correção dos erros do modelo durante o treinamento. Valores menores de learning_rate levam a uma correção mais gradual dos erros, enquanto valores maiores podem levar a uma correção mais rápida, mas também aumentam o risco de overfitting.\n",
    "\n",
    "3. **max_depth:**\n",
    "   - Reescrita: O max_depth define a profundidade máxima das árvores de decisão que são utilizadas como estimadores base no GBM. Árvores mais profundas podem capturar relações mais complexas nos dados, mas também aumentam o risco de overfitting.\n",
    "\n",
    "4. **min_samples_split:**\n",
    "   - Reescrita: O min_samples_split define o número mínimo de amostras necessárias em um nó interno da árvore de decisão para que ocorra uma divisão. Isso ajuda a controlar a complexidade das árvores e a evitar o overfitting.\n",
    "\n",
    "5. **subsample:**\n",
    "   - Reescrita: O subsample determina a fração de amostras a serem usadas em cada estimador do GBM. Usar uma fração menor de amostras pode ajudar a reduzir o overfitting e a melhorar a generalização do modelo, especialmente em conjuntos de dados grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe901606-ffb9-4dbb-970a-283d794a8c85",
   "metadata": {},
   "source": [
    "4 -  (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d2eac1-dcba-4d9d-8054-e4932621c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'learning_rate': 0.5, 'max_depth': 1, 'n_estimators': 150}\n",
      "Acurácia do modelo com melhores parâmetros: 0.9128\n",
      "CPU times: total: 2min 50s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Gerar o conjunto de dados\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Criar o classificador GradientBoosting\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Definir os parâmetros para grid search\n",
    "parameters = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Criar um objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=parameters, cv=5)\n",
    "\n",
    "# Treinar o modelo com grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obter os melhores parâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores parâmetros encontrados:\")\n",
    "print(best_params)\n",
    "\n",
    "# Fazer previsões no conjunto de teste com os melhores parâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "accuracy = best_model.score(X_test, y_test)\n",
    "print(f\"Acurácia do modelo com melhores parâmetros: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85168853-60bc-4f0d-8ebe-5cb831cfd6f5",
   "metadata": {},
   "source": [
    "5 - Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e56b1e-4ecf-49f0-aeda-2d6521c1d9a8",
   "metadata": {},
   "source": [
    "**GBM clássico**\n",
    "\n",
    "No GBM clássico, as atualizações nos pesos dos erros residuais são feitas de forma determinística, pois obtêm todos os exemplos de treinamento em cada iteração para calcular as direções de atualização dos estimadores fracos.\n",
    "\n",
    "Portanto, o GBM clássico leva todos os exemplos de treinamento ao mesmo tempo durante cada etapa de treinamento.\n",
    "\n",
    "**Stochastic GBM**\n",
    "\n",
    "No Stochastic GBM, as atualizações nos pesos dos erros residuais são feitas estocasticamente, ou seja, apenas um subconjunto aleatório dos exemplos de treinamento é escolhido em cada iteração para calcular a atualização deles.\n",
    "\n",
    "Este subconjunto é escolhido aleatoriamente do restante dos dados de treinamento com ou sem reposição, dependendo da implementação específica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
