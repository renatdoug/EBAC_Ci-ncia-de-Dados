{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6543c086-b1d4-44f0-a562-3a23ba36d606",
   "metadata": {},
   "source": [
    "1 - Cite 5 diferenças entre o Random Forest e o AdaBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1301e2-38c9-49c7-8e78-6ac853284980",
   "metadata": {},
   "source": [
    "1. **Algoritmo base:**\n",
    "   - **AdaBoost** é um algoritmo de aprendizado de máquina ensemble que se concentra em melhorar o desempenho combinado de vários classificadores fracos, como árvores (Stumps) de decisão com pouca profundidade (profundidade 1). \n",
    "   - **Random Forest** é um algoritmo de aprendizado de máquina ensemble que cria múltiplas árvores de decisão independentes durante o treinamento e combina suas previsões para obter uma previsão final.\n",
    "\n",
    "2. **Peso das amostras:**\n",
    "   - **AdaBoost** atribui pesos às amostras durante o treinamento, dando mais importância às amostras mal classificadas para que o próximo classificador se concentre nelas.\n",
    "   - **Random Forest** não atribui pesos às amostras; cada árvore é treinada em um subconjunto aleatório de dados, e todas as amostras têm igual importância no processo de treinamento.\n",
    "\n",
    "3. **Método de combinação de previsões:**\n",
    "   - **AdaBoost** combina as previsões dos classificadores fracos usando uma combinação ponderada, onde classificadores com melhor desempenho têm mais influência na previsão final.\n",
    "   - **Random Forest** combina as previsões de várias árvores de decisão por meio de votação majoritária, onde a classe mais votada é escolhida como a previsão final.\n",
    "\n",
    "4. **Sensibilidade a outliers:**\n",
    "   - **AdaBoost** é sensível a outliers, pois atribui pesos às amostras mal classificadas para corrigir erros, o que pode levar a um ajuste excessivo se os outliers não forem tratados adequadamente.\n",
    "   - **Random Forest** é menos sensível a outliers devido ao processo de amostragem aleatória e à votação majoritária, que reduz a influência de amostras individuais na previsão final.\n",
    "\n",
    "5. **Interpretabilidade:**\n",
    "   - **AdaBoost** pode ser menos intuitivo em termos de interpretabilidade devido à complexidade do processo de atribuição de pesos e combinação de previsões dos classificadores fracos.\n",
    "   - **Random Forest** é relativamente mais fácil de interpretar, pois cada árvore de decisão individual pode ser visualizada e compreendida separadamente, além de fornecer medidas de importância de variáveis para ajudar na interpretação do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56074e88-aae2-4f94-a3db-9577584038b5",
   "metadata": {},
   "source": [
    "2 - Acesse o link Scikit-learn – adaboost, leia a explicação (traduza se for preciso) e crie um  jupyter notebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e9d5f7-b6ca-4fd0-8074-99d2da461802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\",)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc6dbc-64f2-45f0-98b9-9485a393d9e9",
   "metadata": {},
   "source": [
    "3 - Cite 5 Hyperparametros importantes no AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445e86d-d0c3-4787-b927-f8a927a01842",
   "metadata": {},
   "source": [
    "**class sklearn.ensemble.AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)**\n",
    "\n",
    "* **estimator** - > valor padrão =None. Este parâmetro representa o estimador base que será usado pelo AdaBoost para construir o conjunto de estimadores.  O valor padrão é None, o que significa que o estimador base padrão será utilizado, geralmente uma árvore de decisão com pouca profundidade (DecisionTreeClassifier(max_depth=1)).\n",
    "  \n",
    "* **n_estimators** - > valor padrão =50. Este parâmetro define o número de estimadores (classificadores fracos) que serão combinados para formar o modelo final do AdaBoost. Um número maior de estimadores pode aumentar a capacidade do modelo de se ajustar aos dados de treinamento, mas também pode aumentar o tempo de treinamento e o risco de overfitting.\n",
    "  \n",
    "* **learning_rate** - > valor padrão =1.0. O learning_rate controla a contribuição de cada estimador para a correção dos erros do modelo durante o treinamento. Valores menores de learning_rate levam a uma correção mais gradual dos erros e geralmente requerem um número maior de estimadores para alcançar um bom desempenho. Por outro lado, valores maiores podem levar a um treinamento mais rápido, mas também aumentam o risco de overfitting.\n",
    "  \n",
    "* **algorithm** -> O algoritmo determina como os pesos das amostras são atualizados durante o treinamento. As opções comuns são 'SAMME' (Stagewise Additive Modeling using a Multi-class Exponential loss function) e 'SAMME.R' (SAMME Real), sendo este último uma versão mais suave do primeiro que usa probabilidades relativas para atualização de pesos.\n",
    "  \n",
    "* **random_state** - > valor padrão = None. Controla a contribuição de cada estimador para a correção dos erros do modelo. Quando um valor específico é fornecido, ele garante que o treinamento seja reproduzível e produza resultados consistentes em diferentes execuções. No entanto, se deixado como None, o processo de treinamento usará uma semente aleatória diferente a cada vez, o que pode resultar em resultados variáveis em diferentes execuções.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde3664-9d5b-4e70-bfe8-3adab93b7fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe901606-ffb9-4dbb-970a-283d794a8c85",
   "metadata": {},
   "source": [
    "4 -  (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d2eac1-dcba-4d9d-8054-e4932621c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados:\n",
      "{'learning_rate': 1.0, 'n_estimators': 50}\n",
      "Acurácia do AdaBoost com melhores parâmetros: 1.0\n",
      "CPU times: total: 5.3 s\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o conjunto de dados Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar um classificador AdaBoost\n",
    "ada_boost = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Definir os parâmetros para grid search\n",
    "parameters = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0]\n",
    "    \n",
    "}\n",
    "\n",
    "# Criar um objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=ada_boost, param_grid=parameters, cv=5)\n",
    "\n",
    "# Treinar o modelo com grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obter os melhores parâmetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores parâmetros encontrados:\")\n",
    "print(best_params)\n",
    "\n",
    "# Fazer previsões no conjunto de teste com os melhores parâmetros\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia do AdaBoost com melhores parâmetros: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
